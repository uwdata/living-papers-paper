<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0"/>
    <meta charset="utf-8" />
    <title>Fast & Accurate Gaussian Kernel Density Estimation</title>
    <meta property="og:title" content="Fast & Accurate Gaussian Kernel Density Estimation" />
    <meta property="og:type" content="article" />
    <meta property="og:description" content="Fast & Accurate Gaussian Kernel Density Estimation" />
    <meta property="description" content="Fast & Accurate Gaussian Kernel Density Estimation" />
    <link rel="stylesheet" href="./bundle.css" />
  </head>
  <body>
    <article><header><h1 role="banner">Fast &amp; Accurate Gaussian Kernel Density Estimation</h1></header><aside><p>Replica of an IEEE VIS 2021 short paper.</p></aside><figure id="kde_impulse" class="teaser" data-counter="1"><img src="figures/impulse_1d.svg"></img><figcaption data-counter="1">Gaussian kernel density estimation for a single impulse value (<tex-math mode="inline" code="m"></tex-math> = 512 bins, <tex-math mode="inline" code="\sigma"></tex-math> = 0.2). Iterated uniform (<q>box</q>) filters <span class="cite-list">[<cite-ref key="Gwosdek:2011" index="8"></cite-ref>, <cite-ref key="Wells:1986" index="26"></cite-ref>]</span> (red &amp; dashed) underestimate the mode and overestimate the sides of the distribution. Deriche’s <span class="cite-list">[<cite-ref key="Deriche:1990" index="4"></cite-ref>, <cite-ref key="Deriche:1993" index="5"></cite-ref>]</span> linear-time recursive filter approximation (blue) produces a pixel-perfect match to the true distribution (grey).</figcaption></figure><h1 nonumber="true">Abstract</h1><p>Kernel density estimation (KDE) models a discrete sample of data as a continuous distribution, supporting the construction of visualizations such as violin plots, heatmaps, and contour plots.
This paper draws on the statistics and image processing literature to survey efficient and scalable density estimation techniques for the common case of Gaussian kernel functions.
We evaluate the accuracy and running time of these methods across multiple visualization contexts and find that the combination of linear binning and a recursive filter approximation by Deriche efficiently produces pixel-perfect estimates across a compelling range of kernel bandwidths.</p><h1 id="introduction" data-counter="1">Introduction</h1><p>Kernel density estimation (<em>KDE</em>) <span class="cite-list">[<cite-ref key="Parzen:1962" index="14"></cite-ref>, <cite-ref key="Rosenblatt:1956" index="17"></cite-ref>]</span> estimates a continuous probability density function for a finite sample of data.
KDE is regularly used to visualize univariate distributions for exploratory analysis in the form of area charts or violin plots <span class="cite-list">[<cite-ref key="Correll:2014" index="3"></cite-ref>, <cite-ref key="Hintzel:1998" index="9"></cite-ref>]</span>, providing valuable alternatives to histograms.
In two dimensions, KDE estimates produce smoothed heatmaps that can be visualized directly as images or used to extract density isolines <span class="cite-list">[<cite-ref key="Lorensen:1987:MCA" index="13"></cite-ref>]</span> for contour plots.</p><p>To form a density estimate, each data point is modeled as a probability distribution, or <em>kernel</em>, centered at that point.
The kernel is parameterized by a <em>bandwidth</em> <tex-math mode="inline" code="\sigma"></tex-math> that determines the width (or spread) of each point distribution.
The sum of these kernels constitutes the density estimate for the sample.
While a variety of kernel functions exist, the normal (Gaussian) distribution is a common choice <span class="cite-list">[<cite-ref key="Scott:1992" index="21"></cite-ref>]</span>, in which case the bandwidth <tex-math mode="inline" code="\sigma"></tex-math> is its standard deviation.</p><p>We would like density estimation to be <em>fast</em>: scalable to large datasets, yet amenable to interactive bandwidth adjustment.
A naïve calculation has quadratic <tex-math mode="inline" code="O(m * n)"></tex-math> complexity: we must sum the contributions of <tex-math mode="inline" code="n"></tex-math> data points at each of <tex-math mode="inline" code="m"></tex-math> locations at which we measure the density.
While approximation methods exist, we want them to be <em>accurate</em>, as inaccurate estimates can result in visualizations with missing features or false local extrema (peaks or valleys).</p><p>This paper reviews scalable, linear-time approximations of Gaussian kernel densities that smooth a binned grid of values.
We evaluate a set of methods – <em>box filters</em> <span class="cite-list">[<cite-ref key="Wand:1994" index="25"></cite-ref>]</span>, <em>extended box filters</em> <span class="cite-list">[<cite-ref key="Gwosdek:2011" index="8"></cite-ref>]</span>, and <em>Deriche’s approximation</em> <span class="cite-list">[<cite-ref key="Deriche:1990" index="4"></cite-ref>, <cite-ref key="Deriche:1993" index="5"></cite-ref>]</span> – in the context of 1D area charts and 2D heatmaps.
We find that the combination of linear binning (proportionally dividing the weight of a point among adjacent bins) and Deriche’s approximation is both fast and highly accurate, outperforming methods currently used in existing visualization tools and often providing pixel-perfect results.</p><h1 id="density-estimation-methods" data-counter="2">Density Estimation Methods</h1><p>Given a dataset with <tex-math mode="inline" code="n"></tex-math> data points <tex-math mode="inline" code="x_i \in \Re"></tex-math>, a kernel function <tex-math mode="inline" code="K"></tex-math>, and bandwidth <tex-math mode="inline" code="\sigma"></tex-math>, the univariate kernel density estimate is defined as:</p><tex-equation data-counter="1">f(x) = \frac{1}{n\sigma} \sum_{i=1}^{n} K{\Big (}\frac{x - x_i}{\sigma}{\Big )}</tex-equation><p>We focus on the case where <tex-math mode="inline" code="K"></tex-math> is the normal (Gaussian) density <tex-math mode="inline" code="K(x) = \frac{1}{\sqrt {2\pi}} e^{-{x^2} / 2}"></tex-math>.
We can directly calculate the density at a point <tex-math mode="inline" code="x"></tex-math> by summing the kernel response for each data point.
If we query the density at <tex-math mode="inline" code="m"></tex-math> measurement points, this approach has computational complexity <tex-math mode="inline" code="O(m * n)"></tex-math>, which for large datasets can be untenable.</p><p>Nevertheless, direct calculation is used by multiple tools.
At the time of writing, Vega and Vega-Lite <span class="cite-list">[<cite-ref key="Satyanarayan:2015" index="18"></cite-ref>, <cite-ref key="Satyanarayan:2016" index="19"></cite-ref>]</span> use direct calculation for one-dimensional KDE, where <tex-math mode="inline" code="m"></tex-math> is the number of points queried in order to draw the density.
Line segments then connect these measured values.
The <code>kde2d</code> function of R’s MASS <span class="cite-list">[<cite-ref key="MASS:2002" index="24"></cite-ref>]</span> package (invoked by the popular ggplot2 <span class="cite-list">[<cite-ref key="Wickham:2009" index="27"></cite-ref>]</span> library for 2D density estimates) also uses a direct calculation approach, limiting the feasible number of measurement points for plotting.
One can optimize direct calculation by leveraging spatial indices (e.g., KD trees or Ball trees) to approximate the contribution of suitably distant points <span class="cite-list">[<cite-ref key="Gray:2003" index="7"></cite-ref>]</span>, as done in the Python scikit-learn library <span class="cite-list">[<cite-ref key="ScikitLearn" index="15"></cite-ref>]</span>.
However, the asymptotic complexity remains a superlinear function of <tex-math mode="inline" code="n"></tex-math>.</p><p>To speed estimation, statisticians proposed <em>binned KDE</em> methods <span class="cite-list">[<cite-ref key="Scott:1985" index="20"></cite-ref>]</span> that first aggregate input data into a uniform grid with <tex-math mode="inline" code="m"></tex-math> bins.
KDE then reduces to the signal processing task of smoothing the binned data.
For example, one can directly convolve the binned values with a discrete Gaussian kernel (or <em>filter</em>).
The resulting complexity <tex-math mode="inline" code="O(n + m * w)"></tex-math> is dependent not just on the number of bins <tex-math mode="inline" code="m"></tex-math>, but also the filter width <tex-math mode="inline" code="w"></tex-math>.
Larger bandwidths can result in filter widths on the same order as <tex-math mode="inline" code="m"></tex-math>, for a quadratic running time.</p><p><cite-ref key="Silverman:1982" mode="inline-author" index="23"></cite-ref> instead applies the Fast Fourier Transform (FFT), an approach used by R’s <code>density</code> routine.
A strength of this method is that it can support arbitrary kernel functions: the binned data and a discretized kernel response are separately mapped to the frequency domain using the FFT, the results are multiplied element-wise (convolution in the frequency domain), and an inverse FFT then produces the density estimate.
The complexity is <tex-math mode="inline" code="O(n + m \log m)"></tex-math>, with binning of <tex-math mode="inline" code="n"></tex-math> points followed by FFT calls on <tex-math mode="inline" code="m"></tex-math>-sized grids.</p><p>For even faster estimates, linear-time <tex-math mode="inline" code="O(n + m)"></tex-math> approximations exist.
These are particularly attractive for 2D density estimation, which can be computed using a series of 1D convolutions along every row and every column of a binned 2D grid.
One can approximate 1D Gaussian convolution by iteratively applying a filter of uniform weight, also known as a <strong>box filter</strong>.
<cite-ref key="Wells:1986" mode="inline-author" index="26"></cite-ref> applies this method to density estimation, contributing a formula for the filter length <tex-math mode="inline" code="w"></tex-math> (or equivalently, its radius <tex-math mode="inline" code="r"></tex-math>) as a function of both <tex-math mode="inline" code="\sigma"></tex-math> and the number of filter iterations <tex-math mode="inline" code="k"></tex-math>.
An attractive property of this approach is its simplicity of calculation: <tex-math mode="inline" code="k"></tex-math> iterations of uniform filtering (running sums), followed by a scale adjustment.
Both d3-contour <span class="cite-list">[<cite-ref key="Bostock:2011" index="1"></cite-ref>]</span> and Vega use per-row and per-column box filters for 2D density estimation.</p><p>Box filtering runs in linear-time, but has important nuances.
First, the bandwidth <tex-math mode="inline" code="\sigma"></tex-math> (a continuous value) is discretized to a filter with integer radius <tex-math mode="inline" code="r"></tex-math>, leading to quantization error.
To address this issue, <cite-ref key="Gwosdek:2011" mode="inline-author" index="8"></cite-ref> propose an <strong>extended box filter</strong> that introduces some non-uniformity by adding fractional weight to the endpoints of the filter.
Second, the true grid size is no longer a constant parameter such as <tex-math mode="inline" code="m"></tex-math> = 512 bins.
As iterated filters <q class="single">blur</q> weight into adjacent bins, the grid must be extended on both ends by an offset of <tex-math mode="inline" code="k * r"></tex-math>.
The running time scales as <tex-math mode="inline" code="n + l"></tex-math>, where <tex-math mode="inline" code="l = m + 2k * r"></tex-math>.
As the filter radius <tex-math mode="inline" code="r"></tex-math> is a monotone function of <tex-math mode="inline" code="\sigma"></tex-math> <span class="cite-list">[<cite-ref key="Wells:1986" index="26"></cite-ref>]</span>, this can result in larger grids – and thus higher time and memory costs – for larger bandwidths.</p><p>Finally, we consider an approximation developed by <strong>Deriche</strong> <span class="cite-list">[<cite-ref key="Deriche:1990" index="4"></cite-ref>, <cite-ref key="Deriche:1993" index="5"></cite-ref>]</span> for computer vision applications.
Deriche models the right half of the standard Gaussian using an order-<tex-math mode="inline" code="K"></tex-math> approximation:</p><tex-equation data-counter="2">h_{K}(x) = \frac{1}{\sqrt{2 \pi \sigma^2}}
\sum_{k=1}^{K} \alpha_k e^{-x \lambda_k / \sigma}</tex-equation><p>From this formulation, Deriche constructs a recursive filter that proceeds linearly from the first value to the last. The left half of the Gaussian is defined by reversing the equation and subtracting the sample at <tex-math mode="inline" code="x = 0"></tex-math> (so that it is not included twice) to form a second filter. We use a fourth-order approximation (<tex-math mode="inline" code="K = 4"></tex-math>), with coefficients</p><tex-equation nonumber="true">\alpha_1 &amp; = 0.84 + i 1.8675, \; &amp; \alpha_3 &amp; = -0.34015 - i 0.1299 \\
\lambda_1 &amp; = 1.783 + i 0.6318, \; &amp; \lambda_3  &amp;= 1.723 + i 1.997</tex-equation><p>and <tex-math mode="inline" code="\alpha_{2k} = \alpha_{2k-1}^{*}"></tex-math>, <tex-math mode="inline" code="\lambda_{2k} = \lambda_{2k-1}^{*}"></tex-math>, where <tex-math mode="inline" code="x^*"></tex-math> denotes the complex conjugate.
Deriche determined the <tex-math mode="inline" code="\alpha_k"></tex-math> and <tex-math mode="inline" code="\lambda_k"></tex-math> parameters using numerical optimization to find the <tex-math mode="inline" code="\ell^2"></tex-math>-best fit to the Gaussian over the domain <tex-math mode="inline" code="n = 0, \dots, 1000"></tex-math> with <tex-math mode="inline" code="\sigma = 100"></tex-math>.
<cite-ref key="Getreuer:2013" mode="inline-author" index="6"></cite-ref> describes how to algebraically rearrange the terms of these filters into direct summations.</p><p>After a constant time initialization to compute summation terms for a chosen <tex-math mode="inline" code="\sigma"></tex-math>, the algorithm requires a linear pass over the <tex-math mode="inline" code="m"></tex-math> bins for each filter, plus a final pass to sum their results.
To handle boundary values, the algorithm must in general perform iterative initialization per filter, requiring at most another linear pass.
Fortunately, this initialization reduces to a constant time operation for zero-padded data (i.e., where no weight resides outside the bins).<inline-note>In contrast to zero-padded data, one must perform iterations for reflected signals (used in image processing to blur without decreasing image brightness) or periodic domains (where the final bin wraps back to the first).</inline-note>
Deriche’s method has complexity <tex-math mode="inline" code="O(n + m)"></tex-math>; it involves more arithmetic operations per step than box filters, but does not require padding the binned grid.
As we will see, it is also much more accurate.
To the best of our knowledge, this work is the first to apply Deriche’s approximation to the task of kernel density estimation.</p><p>While other methods for approximating Gaussian convolution have been proposed, they exhibit higher error rates and/or longer running times than those above.
For more details, see Getreuer’s survey and evaluation in the context of image filtering <span class="cite-list">[<cite-ref key="Getreuer:2013" index="6"></cite-ref>]</span>.</p><h1 id="binning-schemes" data-counter="3">Binning Schemes</h1><p>For binned KDE approaches one must choose how to bin input data into a uniform grid.
By default we assume the weight of a data point is 1; however, a binned grid can easily accommodate variably-weighted points.
<strong>Simple binning</strong>, commonly performed for histograms, places all the weight for a data point into the single bin interval that contains the point.
<strong>Linear binning</strong> <span class="cite-list">[<cite-ref key="Jones:1983" index="11"></cite-ref>, <cite-ref key="Wand:1994" index="25"></cite-ref>]</span> is an alternative that proportionally distributes the weight of a point between adjacent bins.
If a data point <tex-math mode="inline" code="x_i"></tex-math> lies between bins with midpoints <tex-math mode="inline" code="b_0"></tex-math> and <tex-math mode="inline" code="b_1"></tex-math>, linear binning assigns weight proportional to <tex-math mode="inline" code="(b_1 - x_i) / (b_1 - b_0)"></tex-math> to bin <tex-math mode="inline" code="b_0"></tex-math> and <tex-math mode="inline" code="(x_i - b_0) / (b_1 - b_0)"></tex-math> to bin <tex-math mode="inline" code="b_1"></tex-math>.
For example when a point lies at the exact center of a bin, that bin receives all the weight.
If a point resides near the boundary of two bins, its weight is split nearly evenly between them.
We examine both binning approaches in our evaluation below.</p><h1 id="evaluation" data-counter="4">Evaluation</h1><p>How well do the above estimation methods perform in practice?
<cite-ref key="Getreuer:2013" mode="inline-author" index="6"></cite-ref> evaluates a suite of Gaussian filtering methods in the context of image processing (e.g., blurring), inspiring the choice of methods we evaluate here.
That survey does not address the question of binning method (images are already discretized into pixels) and assumes reflected signals outside the image boundaries (for filters that preserve overall image brightness).
<cite-ref key="Bullman:2018" mode="inline-author" index="2"></cite-ref> examine approximate KDE methods for sensor fusion applications.
They do not evaluate visualization directly, and only assess box filter approaches, omitting alternative methods such as Deriche approximation.</p><p>We evaluate KDE methods in a visualization context, assessing box filters, extended box filters, and Deriche approximation.
For the box filter methods, we use <tex-math mode="inline" code="k"></tex-math> = 3 iterations of filtering.
Using 4 or 5 iterations trades-off longer running times for higher accuracy, but the results remain qualitatively similar.
For the Deriche method, we use a 4th-order recursive filter approximation <span class="cite-list">[<cite-ref key="Getreuer:2013" index="6"></cite-ref>]</span>. Datasets and benchmark scripts are included as supplemental material.</p><h2 id="method" data-counter="4.1">Method</h2><p>We compare the speed and accuracy of KDE methods for both univariate and bivariate visualizations.
To measure accuracy, we compare against a direct calculation approach.
As pixel-based visualizations are inherently discrete, we compute <q class="single">ground truth</q> density estimates at the per-pixel level.
We treat each pixel as a bin and calculate the probability mass it contains, which is the difference in the KDE cumulative distribution function between the ending and starting bin boundaries.
We then compare these values to estimates from approximation methods.
For the approximate methods, we linearly interpolate results across the <tex-math mode="inline" code="m"></tex-math> bins to produce pixel-level estimates; this matches the standard plotting method of connecting density measurement points with line segments.</p><p>To evaluate accuracy in a visualization context, we consider how density plots are presented.
It is common to use a linear scale with a domain that ranges from zero to the maximum density.
To mirror this, prior to comparison we separately scale the density estimates, dividing each by its maximum value.
We then multiply by a factor of 100, so that estimates lie on a [0, 100] scale.
This provides an interpretable measure of error: discrepancies between methods correspond to the number of pixels difference in a 100 pixel-tall chart (a reasonable size for showing distributions, including violin plots), and simultaneously conveys a percentage difference.</p><p>We report the maximum (<tex-math mode="inline" code="L_\infty"></tex-math>) error, indicating the most <q class="single">glaring</q> mistake a method makes; root-mean-square error gives qualitatively similar results.
For 1D estimation we compare to ground truth estimates for a 1024 pixel wide chart.
For 2D estimation we compare to ground truth for a 512 <tex-math mode="inline" code="\times"></tex-math> 512 heatmap.
Both were chosen to align with common resolutions and computation constraints.</p><p>Automatic bandwidth selection for kernel density estimation has received a great deal of attention <span class="cite-list">[<cite-ref key="Scott:1992" index="21"></cite-ref>, <cite-ref key="Sheather:1991" index="22"></cite-ref>]</span>.
To contextualize our results, we use Scott’s normal reference density (NRD) rule <span class="cite-list">[<cite-ref key="Scott:1992" index="21"></cite-ref>]</span>, a common default intended to minimize the asymptotic mean integrated squared error (MISE) relative to a normal distribution.</p><p>Each method was implemented as a single-threaded routine in JavaScript for web-based visualization.
Benchmarks were run in Node.js v15.12.0 on a 2017 MacBook Pro with a 2.9 GHz Intel Core i7 processor.
We used the <code>performance.now</code> method of the <code>perf_hooks</code> package to measure running times over repeated runs.</p><h2 id="results-1d-estimation" data-counter="4.2">Results: 1D Estimation</h2><figure id="err1d_impulse" class="figure latex:page" position="t" data-counter="2"><img width="100%" src="figures/error_1d_impulse.svg"></img><figcaption data-counter="2">1D estimation error for a single impulse. Error (plotted on a log scale) is measured as the maximum pixel error given a 100-pixel plot height. Box filters exhibit an oscillating pattern due to bandwidth quantization; the extended box method smooths these artifacts. Deriche approximation consistently produces lower error, typically with sub-pixel accuracy. Linear binning further reduces the error rate.</figcaption></figure><figure id="err1d_penguins" class="figure latex:page" position="t" data-counter="3"><img width="100%" src="figures/error_1d_penguins.svg"></img><figcaption data-counter="3">1D estimation error for Gentoo penguin body mass. Error (plotted on a log scale) is measured as the maximum pixel error given a 100-pixel plot height. Dashed gray lines indicate the normal reference density (NRD) heuristic for automatic bandwidth (<tex-math mode="inline" code="\sigma"></tex-math>) selection <span class="cite-list">[<cite-ref key="Scott:1992" index="21"></cite-ref>]</span>. The combination of linear binning and Deriche approximation consistently produces the most accurate estimates.</figcaption></figure><figure id="kde_penguins" class="figure margin" position="t" data-counter="4"><img src="figures/penguins_1d.svg"></img><figcaption data-counter="4">KDE of Gentoo penguin body mass (<tex-math mode="inline" code="m"></tex-math> = 512 bins, <tex-math mode="inline" code="\sigma"></tex-math> = 50). Box filters tend to underestimate peaks and overestimate valleys, in some cases <q class="single">eroding</q> local peaks (e.g., around 4.9k &amp; 5.7k grams). Deriche approximation instead produces a pixel-perfect result.</figcaption></figure><p>We first evaluate the KDE methods relative to an impulse: we locate a single point at <tex-math mode="inline" code="x = 0"></tex-math> and perform estimation over the domain <tex-math mode="inline" code="[-1, 1]"></tex-math>.
The result should be a Gaussian distribution with mean 0 and standard deviation matching the kernel bandwidth <tex-math mode="inline" code="\sigma"></tex-math>.
<cross-ref type="fig" xref="kde_impulse" index="1"></cross-ref> shows the result for <tex-math mode="inline" code="\sigma"></tex-math> = 0.2 and <tex-math mode="inline" code="m"></tex-math> = 512 bins (sans re-scaling).
The box filter methods produce perceptible errors, whereas Deriche approximation provides a pixel-perfect match to the actual distribution.</p><p><cross-ref type="fig" xref="err1d_impulse" index="2"></cross-ref> presents scaled comparisons by binning scheme, bin count, and bandwidth.
Standard box filters produce oscillating errors due to bandwidth quantization.
The extended box method smooths these artifacts.
Deriche approximation consistently produces the lowest error, and notably improves with the use of linear binning.</p><p>We next examine real-world measurements from the Palmer Penguins dataset <span class="cite-list">[<cite-ref key="Horst:2020" index="10"></cite-ref>]</span>.
We estimate densities for penguin body mass (in grams) for <tex-math mode="inline" code="n"></tex-math> = 123 Gentoo penguins on the domain <tex-math mode="inline" code="[0, 7000]"></tex-math>.
<cross-ref type="fig" xref="err1d_penguins" index="3"></cross-ref> shows maximum estimation errors.
We again see that the combination of Deriche approximation and linear binning produces the best results, often with sub-pixel error.
<cross-ref type="fig" xref="kde_penguins" index="4"></cross-ref> shows a subset of the visualized density.
The box filter methods again produce perceptible deviations, which in multiple instances obscure local extrema.
Deriche approximation produces a pixel-perfect result.</p><figure id="time1d_penguins" class="figure margin" position="t" data-counter="5"><img width="100%" src="figures/time_1d_penguins.svg"></img><figcaption data-counter="5">Running time of 1D estimation on resampled penguin data (<tex-math mode="inline" code="m"></tex-math> = 512 bins). As <tex-math mode="inline" code="n"></tex-math> increases, the running time of the approximation methods is dominated by the <tex-math mode="inline" code="O(n)"></tex-math> binning cost.</figcaption></figure><p>To assess scalability, we generate datasets of arbitrary size based on the Gentoo penguins data.
We first fit a kernel density estimate using direct calculation (<tex-math mode="inline" code="\sigma"></tex-math> = 200, based on the NRD value of 204.11), then sample from the resulting distribution to generate datasets ranging from <tex-math mode="inline" code="n"></tex-math> = 100 to <tex-math mode="inline" code="n"></tex-math> = 10M points.
Each timing measurement is taken for <tex-math mode="inline" code="m"></tex-math> = 512 bins and averages runs for five bandwidths (100, 150, 200, 250, 300) centered near the original NRD value.
<cross-ref type="fig" xref="time1d_penguins" index="5"></cross-ref> plots the results.
For small <tex-math mode="inline" code="n"></tex-math>, box filtering is slightly faster as it involves fewer arithmetic operations.
As <tex-math mode="inline" code="n"></tex-math> increases, the <tex-math mode="inline" code="O(n)"></tex-math> binning calculation dominates and all methods exhibit similar performance.</p><h2 id="results-2d-estimation" data-counter="4.3">Results: 2D Estimation</h2><figure id="err2d_cars" class="figure latex:page" position="t" data-counter="6"><img width="100%" src="figures/error_2d_cars.svg"></img><figcaption data-counter="6">2D estimation error for car data. Error (on a log scale) is measured as the maximum pixel error given a 100-pixel plot height. Dashed gray lines indicate the NRD <tex-math mode="inline" code="\sigma"></tex-math> value. With 512 bins and linear binning, the Deriche method results in sub-pixel accuracy at all sampled bandwidths.</figcaption></figure><figure id="contours" class="figure margin" position="h!" data-counter="7"><img src="figures/cars_2d_multiples.svg"></img>
<img src="figures/cars_2d_overlap.svg"></img>
<figcaption data-counter="7">Heatmaps and contour plots of car data (miles per gallon vs. horsepower). <em>Top:</em> plots per density method. <em>Bottom:</em> contour lines per method overlaid for comparison. Deriche’s approximation matches the precise density estimate. Box filters result in extra or missing contour lines and distorted shapes.</figcaption></figure><p>To assess bivariate estimates, we use the classic cars dataset <span class="cite-list">[<cite-ref key="CarsData" index="16"></cite-ref>]</span> and examine the relationship between mileage and horsepower.
We use the same error measure.
<cross-ref type="fig" xref="err2d_cars" index="6"></cross-ref> presents the error across binning and bandwidth choices (the same bandwidth is used for the x- and y-dimensions), with similar patterns as before.
Deriche approximation with linear binning at <tex-math mode="inline" code="m"></tex-math> = 512 bins produces notably low error rates.</p><p><cross-ref type="fig" xref="contours" index="7"></cross-ref> shows heatmaps and contour plots for 2D density estimation, both as separate plots and as contours overlaid on a ground truth heatmap.
We set <tex-math mode="inline" code="\sigma"></tex-math> = 0.04 for both the x- and y-dimensions, near the NRD estimates (0.049, 0.048) for each variable.
The same contour thresholds – <tex-math mode="inline" code="[0.01, 0.08]"></tex-math> with increments of 0.01 – are applied for each method.
Comparison of the contour plots reveals hallucinators <span class="cite-list">[<cite-ref key="Kindlmann:2014" index="12"></cite-ref>]</span>, where approximation methods produce different visual features for the same underlying data.
The Deriche method provides a pixel-perfect match to the true density, but the box filter methods result in different extrema as well as distorted contour shapes.</p><p>As shown earlier (<cross-ref type="fig" xref="time1d_penguins" index="5"></cross-ref>), for large datasets the running time of binned KDE methods is dominated by the <tex-math mode="inline" code="O(n)"></tex-math> binning.
Here we instead assess the effect of bandwidth on 2D estimation time, shown in <cross-ref type="fig" xref="time2d_cars" index="8"></cross-ref>.
At low bandwidths, standard box filtering is fastest due to fewer operations per bin.
However, both box filter methods become slower at larger bandwidths due to the need to enlarge the underlying grid.
This overhead is exacerbated for 2D estimation, as the number of expanded cells multiply across grid rows and columns.
In contrast the Deriche method is stable across bandwidths as it does not require grid extensions, with performance matching or exceeding the other methods for bandwidths at or above the NRD bandwidth suggestion.</p><figure id="time2d_cars" class="figure" position="h!" data-counter="8"><img width="70%" src="figures/time_2d_cars.svg"></img><figcaption data-counter="8">Running time of 2D estimation on car data, by bandwidth. At low bandwidths, Deriche’s method is slightly slower due to more arithmetic operations. As the bandwidth increases, the box filters require larger grids, leading to longer running times.</figcaption></figure><h1 id="conclusion" data-counter="5">Conclusion</h1><p>We survey approaches for KDE, finding that a combination of linear binning and Deriche approximation results in fast, linear-time performance and excellent accuracy at all but the smallest bandwidths.
Though limited to Gaussian kernels only, this approach provides fast and accurate KDE for the tested univariate and bivariate visualizations.
Our implementation and benchmarks, including additional error metrics, are available as open source software (<a class="uri" href="https://github.com/uwdata/fast-kde">https://github.com/uwdata/fast-kde</a>) and we intend to integrate this method into existing web-based visualization libraries.</p><h1 nonumber="true">Acknowledgments</h1><p>We thank the UW Interactive Data Lab and anonymous reviewers.
The work was supported by a Moore Foundation software grant.</p><h1 nonumber="true">References</h1><ol class="references"><li id="ref-0">Bostock, M., Ogievetsky, V., &amp; Heer, J. (2011). D3: Data-Driven Documents. IEEE Transactions on Visualization and Computer Graphics, 17(12), 2301–2309. <a href="https://doi.org/10.1109/TVCG.2011.185">https://doi.org/10.1109/TVCG.2011.185</a></li><li id="ref-1">Bullmann, M., Fetzer, T., Ebner, F., Deinzer, F., &amp; Grzegorzek, M. (2018). Fast Kernel Density Estimation Using Gaussian Filter Approximation. Proc. International Conference on Information Fusion (FUSION), 1233–1240. <a href="https://doi.org/10.23919/ICIF.2018.8455686">https://doi.org/10.23919/ICIF.2018.8455686</a></li><li id="ref-2">Correll, M., &amp; Gleicher, M. (2014). Error Bars Considered Harmful: Exploring Alternate Encodings for Mean and Error. IEEE Transactions on Visualization and Computer Graphics, 20(12), 2142–2151. <a href="https://doi.org/10.1109/TVCG.2014.2346298">https://doi.org/10.1109/TVCG.2014.2346298</a></li><li id="ref-3">Deriche, R. (1990). Fast Algorithms for Low-Level Vision. IEEE Transactions on Pattern Analysis and Machine Intelligence, 12(1), 78–87. <a href="https://doi.org/10.1109/34.41386">https://doi.org/10.1109/34.41386</a></li><li id="ref-4">Deriche, R. (1993). Recursively implementing the Gaussian and its derivatives [Techreport]. INRIA. <a href="http://hal.inria.fr/docs/00/07/47/78/PDF/RR-1893.svg">http://hal.inria.fr/docs/00/07/47/78/PDF/RR-1893.svg</a></li><li id="ref-5">Getreuer, P. (2013). A Survey of Gaussian Convolution Algorithms. Image Processing On Line, 3, 286–310. <a href="https://doi.org/10.5201/ipol.2013.87">https://doi.org/10.5201/ipol.2013.87</a></li><li id="ref-6">Gray, A. G., &amp; Moore, A. W. (2003). Nonparametric density estimation: Toward computational tractability. Proc. 2003 SIAM International Conference on Data Mining, 203–211. <a href="https://doi.org/10.1137/1.9781611972733.19">https://doi.org/10.1137/1.9781611972733.19</a></li><li id="ref-7">Gwosdek, P., Grewenig, S., Bruhn, A., &amp; Weickert, J. (2011). Theoretical foundations of Gaussian convolution by extended box filtering. Proc. International Conference on Scale Space and Variational Methods in Computer Vision, 447–458. <a href="https://doi.org/10.1007/978-3-642-24785-9_38">https://doi.org/10.1007/978-3-642-24785-9_38</a></li><li id="ref-8">Hintze, J. L., &amp; Nelson, R. D. (1998). Violin Plots: A Box Plot-Density Trace Synergism. The American Statistician, 52(2), 181–184. <a href="https://doi.org/10.1080/00031305.1998.10480559">https://doi.org/10.1080/00031305.1998.10480559</a></li><li id="ref-9">Horst, A. M., Hill, A. P., &amp; Gorman, K. B. (2020). palmerpenguins: Palmer Archipelago (Antarctica) penguin data. <a href="https://doi.org/10.5281/zenodo.3960218">https://doi.org/10.5281/zenodo.3960218</a></li><li id="ref-10">Jones, M. C., &amp; Lotwick, H. W. (1983). On the errors involved in computing the empirical characteristic function. Journal of Statistical Computation and Simulation, 17(2), 133–149. <a href="https://doi.org/10.1080/00949658308810650">https://doi.org/10.1080/00949658308810650</a></li><li id="ref-11">Kindlmann, G., &amp; Scheidegger, C. (2014). An Algebraic Process for Visualization Design. IEEE Transactions on Visualization and Computer Graphics, 20(12), 2181–2190. <a href="https://doi.org/10.1109/TVCG.2014.2346325">https://doi.org/10.1109/TVCG.2014.2346325</a></li><li id="ref-12">Lorensen, W. E., &amp; Cline, H. E. (1987). Marching Cubes: A High Resolution 3D Surface Construction Algorithm. SIGGRAPH Computer Graphics, 21(4), 163–169. <a href="https://doi.org/10.1145/37402.37422">https://doi.org/10.1145/37402.37422</a></li><li id="ref-13">Parzen, E. (1962). On Estimation of a Probability Density Function and Mode. The Annals of Mathematical Statistics, 33(3), 1065–1076. <a href="https://doi.org/10.1214/aoms/1177704472">https://doi.org/10.1214/aoms/1177704472</a></li><li id="ref-14">Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., &amp; Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12, 2825–2830. <a href="https://doi.org/10.5555/1953048.2078195">https://doi.org/10.5555/1953048.2078195</a></li><li id="ref-15">Ramos, E., &amp; Donoho, D. (1983). ASA Data Exposition Dataset. <a href="http://stat-computing.org/dataexpo/1983.html. http://stat-computing.org/dataexpo/1983.html">http://stat-computing.org/dataexpo/1983.html. http://stat-computing.org/dataexpo/1983.html</a></li><li id="ref-16">Rosenblatt, M. (1956). Remarks on Some Nonparametric Estimates of a Density Function. The Annals of Mathematical Statistics, 27(3), 832–837. <a href="https://doi.org/10.1214/aoms/1177728190">https://doi.org/10.1214/aoms/1177728190</a></li><li id="ref-17">Satyanarayan, A., Russell, R., Hoffswell, J., &amp; Heer, J. (2015). Reactive Vega: A streaming dataflow architecture for declarative interactive visualization. IEEE Transactions on Visualization and Computer Graphics, 22(1), 659–668. <a href="https://doi.org/10.1109/TVCG.2015.2467091">https://doi.org/10.1109/TVCG.2015.2467091</a></li><li id="ref-18">Satyanarayan, A., Moritz, D., Wongsuphasawat, K., &amp; Heer, J. (2016). Vega-Lite: A grammar of interactive graphics. IEEE Transactions on Visualization and Computer Graphics, 23(1), 341–350. <a href="https://doi.org/10.1109/TVCG.2016.2599030">https://doi.org/10.1109/TVCG.2016.2599030</a></li><li id="ref-19">Scott, D. W., &amp; Sheather, S. J. (1985). Kernel density estimation with binned data. Communications in Statistics - Theory and Methods, 14(6), 1353–1359. <a href="https://doi.org/10.1080/03610928508828980">https://doi.org/10.1080/03610928508828980</a></li><li id="ref-20">Scott, D. W. (1992). Multivariate Density Estimation: Theory, Practice, and Visualization. John Wiley &amp; Sons. <a href="https://doi.org/10.1002/9780470316849">https://doi.org/10.1002/9780470316849</a></li><li id="ref-21">Sheather, S. J., &amp; Jones, M. C. (1991). A reliable data-based bandwidth selection method for kernel density estimation. Journal of the Royal Statistical Society, Series B (Methodological), 53(3), 683–690. <a href="https://doi.org/10.1111/j.2517-6161.1991.tb01857.x">https://doi.org/10.1111/j.2517-6161.1991.tb01857.x</a></li><li id="ref-22">Silverman, B. W. (1982). Algorithm AS 176: Kernel density estimation using the fast Fourier transform. Journal of the Royal Statistical Society, Series C (Applied Statistics), 31(1), 93–99. <a href="https://doi.org/10.2307/2347084">https://doi.org/10.2307/2347084</a></li><li id="ref-23">Venables, W. N., &amp; Ripley, B. D. (2002). Modern applied statistics with S. Springer. <a href="https://doi.org/10.1007/978-0-387-21706-2">https://doi.org/10.1007/978-0-387-21706-2</a></li><li id="ref-24">Wand, M. P. (1994). Fast Computation of Multivariate Kernel Estimators. Journal of Statistical Computation and Simulation, 3(4), 433–445. <a href="https://doi.org/10.2307/1390904">https://doi.org/10.2307/1390904</a></li><li id="ref-25">Wells, W. M. (1986). Efficient synthesis of Gaussian filters by cascaded uniform filters. IEEE Transactions on Pattern Analysis and Machine Intelligence, 8(2), 234–239. <a href="https://doi.org/10.1109/TPAMI.1986.4767776">https://doi.org/10.1109/TPAMI.1986.4767776</a></li><li id="ref-26">Wickham, H. (2009). ggplot2: Elegant Graphics for Data Analysis. Springer. <a href="https://doi.org/10.1007/978-0-387-98141-3">https://doi.org/10.1007/978-0-387-98141-3</a></li></ol></article>
    <script type="module" src="./bundle.js"></script>
</html>